{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 5  AC 209 : PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "Names of people you have worked with goes here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise'> <b> Question 5 [25 pts] </b> </div>\n",
    "\n",
    "Suppose we want to conduct PCA on the model matrix $X \\in \\Re^{n√óp}$, where the columns have been suitably set to zero mean. In this question, we consider the squared reconstruction error:\n",
    "\n",
    "$$  \\parallel XQ- XQ_m \\parallel ^2 $$\n",
    "\n",
    "for a suitable set of eigenvectors forming the matrix $Q_m$, as discussed below. Suppose that we conduct eigendecomposition of $X^T X$ and obtain eigenvalues $\\lambda_1, \\ldots , \\lambda_p$ and principal components $Q$, i.e.\n",
    "\n",
    "$$ X^T X = Q \\Lambda Q ^T $$\n",
    "\n",
    "**5.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products.\n",
    "\n",
    "**5.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n",
    "\n",
    "**5.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "**5.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "**5.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "**5.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Suppose that the matrix norm is simply the squared dot product, namely\n",
    "\n",
    "$$ \\parallel A \\parallel ^2 = A^T A $$\n",
    "\n",
    "Then, express the reconstruction error as a sum of matrix products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "$\\parallel XQ - X Q_m \\parallel ^2 = (X \\, Q - X \\,  Q_m)^T \\, (X\\, Q - X \\, Q_m) = ((XQ )^T- (X Q_m)^T) \\, (XQ - X Q_m) $\n",
    "\n",
    "$ =  (Q^T X^T  -  Q_m^T X^T) \\, (XQ - X Q_m)  = Q^T X^T XQ - Q^T X^T  X Q_m  -  Q_m^T X^T XQ + Q_m^T X^T X Q_m $\n",
    "\n",
    " \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2**  Simplify your result from 5.1 based on properties of the matrices $Q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "\n",
    "$ \\parallel XQ - X Q_m \\parallel ^2 =  Q^T Q \\Lambda Q^T Q - Q^T Q \\Lambda Q^T Q_m  -  Q_m^T Q \\Lambda Q^T Q + Q_m^T Q \\Lambda Q^T Q_m $\n",
    "\n",
    "$ = \\Lambda - \\Lambda Q^T Q_m  - \\Lambda Q_m^T Q + Q_m^T Q \\Lambda Q^T Q_m $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** Now let $Q_m$ be the matrix of the first $m < p$ eigenvectors, namely\n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "*your answer here* \n",
    "\n",
    "$$ Q_m = (q_1, \\ldots, q_m, 0, \\ldots, 0) \\in \\Re^{p \\times p} $$\n",
    "\n",
    "Thus, $X Q_m$ is the PCA projection of the data into the space spanned by the first $m$ principal components. Express the products $Q^T_m Q$ and $Q^T Q_m$, again using properties of the eigenbasis $q_1, \\ldots, q_p$.\n",
    "\n",
    "$$Q^TQ_m = I_{mp} = Q_m^TQ$$\n",
    "where $I_{mp}$ is the identity matrix for the first $m$ rows and columns, and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4**  Use your results from 5.3 to finally fully simplify your expression from 5.2.\n",
    "\n",
    "*your answer here* \n",
    "\n",
    "Plugging in these results, we find that we have the products\n",
    "$$\\Lambda Q^TQ_m = \\Lambda_{mp}$$\n",
    "defined similarly for $I_{mp}$, and\n",
    "$$Q_m^TQ\\Lambda Q^TQ_m = \\Lambda_{mp}$$\n",
    "and so the final expression is\n",
    "$$\\Lambda - \\Lambda_{mp}$$\n",
    "which is zero for the first $m$ columns and rows, and has a diagonal of eigenvalues $\\lambda_{m+1}, \\dots, \\lambda_p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** Note that the result you obtain should still be a matrix, i.e. this does not define a proper norm on the space of matrices (since the value should be a scalar). Consequently, the true matrix norm is actually the trace of the\n",
    "above result, namely\n",
    "$$ \\parallel A \\parallel ^2  = {\\rm trace} (A^T A) $$\n",
    "Use your result from 5.4 and this new definition to find a simple expression\n",
    "for the reconstruction error in terms of the eigenvalues.\n",
    "\n",
    "*your answer here* \n",
    "\n",
    "From the above, we see that\n",
    "$$\\text{trace}(\\Lambda - \\Lambda_{mp}) = \\sum_{i=m+1}^p \\lambda_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.6** Interpret your result from (5). In light of your results, does our procedure for PCA (selecting the $m$ substantially larger eigenvalues) make sense? Why or why not?\n",
    "\n",
    "*your answer here*\n",
    "\n",
    "This is simply the sum of the eigenvalues of the components not selected during PCA. This provides another intuitive justification for the PCA procedure, as picking the substantially largest eigenvalues corresponds to minimizing the squared reconstruction error (while still reducing dimensionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
