{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 8  AC 209 : Trees and ensemble methods\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL FOR FORMAT\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Describe the main differences between bagging and adaptative boosting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is a parallel method that uses bootstrap samples to train N weak learners, and then combines the predictions. Adaptative boosting, on the other hand, works sequentially: at each step, a boostrap sample is obtained from a weighted dataset, where the weight of each observation corresponds to its likelihood of being chosen. The weights are updated based on the performance of the previous learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Why do we use the word \"gradient\" in gradient boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is actually performing gradient descent over the specified loss function for our dataset - we are minimizing empirical risk. Gradient boosting is basically gradient descent over prediction space. At each step, we fit a learner $h_m$ to the residuals of the current best model, and we update our classifier with:\n",
    "\n",
    "$$F_m(x)=F_{m-1}(x)+\\eta h_m(x)$$\n",
    "\n",
    "Which corresponds to an approximation of a gradient descent update in prediction space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Describe three improvements of XGBoost over the conventional implementation of Boosted Trees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can use L1 or L2 regularization.\n",
    "- Incorporates a sparsity-aware split finding algorithm to handle different types of sparsity patterns in the data.\n",
    "- Uses distributed weighted quantile sketch algorithm to effectively handle weighted data.\n",
    "- Uses a parallelized gradient boosting algorithm that greatly reduces inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will compare some of the top ensemble methods for classification. We will look at AdaBoost, XGBoost, LGBM and CatBoost. \n",
    "\n",
    "- To install XGBoost, run `pip3 install xgboost`.\n",
    "- To install LGBM, run `pip install lightgbm`\n",
    "- To install CatBoost, run `conda -c conda-forge install catboost` if using conda, or `pip install catboost` if not. \n",
    "\n",
    "We will be using a different dataset than what we're used to, so as to test the capabilities of these advanced classifiers. We will be playing with the Forest Cover Type dataset, a classification dataset where observations from 30mx30m patches of forest are associated with the type of tree that grows there. We will be trying to predict the primary species of those patches based on 54 predictors, e.g. elevation, slope, distance to water, etc.\n",
    "\n",
    "Here are the main predictors of the dataset:\n",
    "- Elevation\n",
    "- Aspect\n",
    "- Slope\n",
    "- Horizontal_Distance_To_Hydrology \n",
    "- Vertical_Distance_To_Hydrology \n",
    "- Hillshade_9am\n",
    "- Hillshade_Noon\n",
    "- Hillshade_3pm\n",
    "- Horizontal_Distance_To_Fire_Points\n",
    "- Wilderness_Area (one-hot encoded, 4 binary columns)\n",
    "- Soil_Type (one-hot encoded, 40 binary columns)\n",
    "\n",
    "Response:\n",
    "Cover_Type (7 types), integer, 1 to 7\n",
    "\n",
    "For more details on the dataset, visit http://archive.ics.uci.edu/ml/datasets/Covertype "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Import the coverage type dataset from sklearn.datasets with `datasets.fetch_covtype`. Use return_X_y=True and split the data into train and test sets. You can downsample the data to 10% of the full dataset if needed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Train a DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier, LGBMClassifier, XGBoostClassifier, and CatBoost on the data.**\n",
    "\n",
    "**Make sure that you use the sklearn-like interfaces:**\n",
    "\n",
    "- DecisionTreeClassifier, RandomForestClassifier, AdaboostClassifier given by sklearn\n",
    "- XGBClassifier can be accessed with `from xgboost import XGBClassifier`\n",
    "- LGBMClassifier can be accessed with `from lightgbm.sklearn import LGBMClassifier`\n",
    "- CatBoostClassifier can be accessed with `from catboost import CatBoostClassifier`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Time both training (.fit method) and inference (.predict method), and show classification accuracy for all classifiers. For this dataset, substract 1 to your array of labels so that the label format plays nicely with CatBoost. Comment on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77855, 54) (77855,) (38347, 54) (38347,)\n"
     ]
    }
   ],
   "source": [
    "X,y = datasets.fetch_covtype(data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=True)\n",
    "\n",
    "X = X[:int(len(X)*0.2)]\n",
    "y = y[:int(len(y)*0.2)]\n",
    "\n",
    "y=y-1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_time(clf, X_train, X_test, y_train, y_test):\n",
    "    # training\n",
    "    start = timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = timer()\n",
    "    train_time = end-start\n",
    "\n",
    "    # inference\n",
    "    start = timer()\n",
    "    clf.predict(X_test)\n",
    "    end = timer()\n",
    "    inf_time = end-start\n",
    "\n",
    "    # accuracy\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(clf.__class__.__name__)\n",
    "    print('Accuracy: %.5f' % acc)\n",
    "    print('Training time: %.5fs' % train_time)\n",
    "    print('Inference time: %.5fs' % inf_time)\n",
    "    \n",
    "    return acc, train_time, inf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Accuracy: 0.93371\n",
      "Training time: 0.98405s\n",
      "Inference time: 0.01637s\n"
     ]
    }
   ],
   "source": [
    "clf =  DecisionTreeClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "Accuracy: 0.76903\n",
      "Training time: 6.79260s\n",
      "Inference time: 0.60482s\n"
     ]
    }
   ],
   "source": [
    "clf =  AdaBoostClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "Accuracy: 0.93548\n",
      "Training time: 1.50544s\n",
      "Inference time: 0.12063s\n"
     ]
    }
   ],
   "source": [
    "clf =  RandomForestClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier\n",
      "Accuracy: 0.98305\n",
      "Training time: 0.13361\n",
      "Inference time: 0.00077\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier\n",
      "Accuracy: 0.92112\n",
      "Training time: 5.76897s\n",
      "Inference time: 1.64111s\n"
     ]
    }
   ],
   "source": [
    "clf = LGBMClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostClassifier\n",
      "Accuracy: 0.81159\n",
      "Training time: 1.73230s\n",
      "Inference time: 0.15873s\n"
     ]
    }
   ],
   "source": [
    "clf = CatBoostClassifier(loss_function='MultiClass', iterations=10, verbose=False)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Let's now play with a high-dimensional dataset. Load the Faces in The Wild dataset with `datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)`. Split the data into train and test sets (30% test). Use random_state=209 for the train test split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2025, 2914) (2025,) (998, 2914) (998,)\n"
     ]
    }
   ],
   "source": [
    "X,y = datasets.fetch_lfw_people(return_X_y=True, min_faces_per_person=20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=209)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Again, train all classifiers enumerated above and time training and inference times. With this dimensionality, you will have to increase the n_jobs parameter of the ensemble methods to reduce training time. Evaluate the algorithms with `n_estimators =10` and `n_jobs=10`. You should expect XGBoost to take a couple of minutes to run. Reduce the number of jobs and estimators to 5 if your machine cannot handle it (but make sure that you remain consistent across estimators). Comment on the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "Accuracy: 0.19840\n",
      "Training time: 13.64126s\n",
      "Inference time: 0.00241s\n"
     ]
    }
   ],
   "source": [
    "clf =  DecisionTreeClassifier()\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier\n",
      "Accuracy: 0.18637\n",
      "Training time: 7.01525s\n",
      "Inference time: 0.03597s\n"
     ]
    }
   ],
   "source": [
    "clf =  AdaBoostClassifier(n_estimators=10)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "Accuracy: 0.29559\n",
      "Training time: 1.58273s\n",
      "Inference time: 0.01612s\n"
     ]
    }
   ],
   "source": [
    "clf =  RandomForestClassifier(n_estimators = 10)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier\n",
      "Accuracy: 0.39679\n",
      "Training time: 48.13600s\n",
      "Inference time: 0.56427s\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(n_estimators=10, n_jobs=20)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier\n",
      "Accuracy: 0.32665\n",
      "Training time: 100.97716s\n",
      "Inference time: 0.02705s\n"
     ]
    }
   ],
   "source": [
    "clf = LGBMClassifier(n_estimators=10, n_jobs=20)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = CatBoostClassifier(loss_function='MultiClass', verbose=False, n_estimators=10)\n",
    "fit_predict_time(clf, X_train, X_test, y_train, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) How did the high dimensionality affect each classifier? Comment on the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the dimensionality reduces the predictive power of our classifiers. The ensemble methods, however, proved to work better than the simple decision tree in this case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
