{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109A Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7: Building a Neural Network and Dealing with missing values\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors:** Pavlos Protopapas and Kevin Rader<br/>\n",
    "**Lab Instructor:** Eleni Kaxiras<br/>\n",
    "**Authors:** David Sondak and Pavlos Protopapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "In this lab, we'll revise matrix operations and talk about tensors. We will also talk about handling missing data.\n",
    "\n",
    "By the end of this lab, you should:\n",
    "\n",
    "- Be able to think of vectors and arrays as tensors.\n",
    "- Understand how a simple neural network works and code its functions from scratch.\n",
    "- Know how to use simple pandas and scikit-learn functions to handle missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Tensors and Neural Networks\n",
    "\n",
    "We can think of tensors as multidimensional arrays of numerical values; their job is to  generalize matrices to multiple dimensions. While tensors first emerged in the 20th century, they have since been applied to numerous other disciplines, including machine learning. Tensor decomposition/factorization can solve, among other, problems in unsupervised learning settings, temporal and multirelational data. When we get to handling images for Convolutional Neural Networks, it's a good idea to have the understanding of tensors of rank 3.\n",
    "\n",
    "We will use the following naming conventions:\n",
    "\n",
    "- scalar = just a number = rank 0 tensor  ($a$ ∈ $F$,)\n",
    "<BR><BR>\n",
    "- vector = 1D array = rank 1 tensor ( $x = (\\;x_1,...,x_i\\;)⊤$ ∈ $F^n$ )\n",
    "<BR><BR>\n",
    "- matrix = 2D array = rank 2 tensor ( $\\textbf{X} = [a_{ij}] ∈ F^{m×n}$ )\n",
    "<BR><BR>\n",
    "- 3D array = rank 3 tensor ( $\\mathscr{X} =[t_{i,j,k}]∈F^{m×n×l}$ )\n",
    "<BR><BR>\n",
    "- $\\mathscr{N}$D array = rank $\\mathscr{N}$ tensor ( $\\mathscr{T} =[t_{i1},...,t_{i\\mathscr{N}}]∈F^{n_1×...×n_\\mathscr{N}}$ ) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python library for tensors\n",
    "Machine learning libraries have special functions to handle tensors. Numpy has `numpy.tensordot`. Feel free to read about it: [numpy.tensordot](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.tensordot.html), but we will **not be covering it** in class.\n",
    "\n",
    "#### Tensor indexing\n",
    "We can create subarrays by fixing some of the given tensor’s indices. We can create a vector by fixing all but one index. A 2D matrix is created when fixing all but two indices. For example, for a third order tensor the vectors are $\\mathscr{X}[:,j,k]$ = $\\mathscr{X}[j,k]$ (column), $\\mathscr{X}[i,:,k]$ = $\\mathscr{X}[i,k]$ (row), and $\\mathscr{X}[i,j,:]$ = $\\mathscr{X}[i,j]$ (tube); the matrices are $\\mathscr{X}[:,:,k]$ (frontal), $\\mathscr{X}[:,j,:]$ (lateral), $\\mathscr{X}[i,:,:]$ (horizontal). \n",
    "#### Tensor multiplication\n",
    "We can multiply one matrix with another as long as the sizes are compatible ((n × m) × (m × p) = n × p), and also multiply an entire matrix by a constant. Numpy `numpy.dot` performs a matrix multiplication which is straightforward when we have 2D or 1D arrays. But what about > 3D arrays? The function will choose according to the matching dimentions but if we want to choose we should use `tensordot`, but, again, we do not need that for this class. \n",
    "\n",
    "In this lab we will be doing a lot of array manipulation, including: \n",
    "- reshaping arrays, \n",
    "- slicing x = x[0, 10:15, 10:15] brings it into a lower rank, reordering. Create some small tensors.  \n",
    "- matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reese Witherspoon as a rank 3 Tensor\n",
    "\n",
    "(photo from [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/person/Reese_Witherspoon.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and show the image\n",
    "img = mpimg.imread('Reese_Witherspoon.jpg')\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type is this image? We have a suspicion that it's an array so let's find out its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img), img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a 24-bit RGB PNG image (height, width, channels) with 8 bits for each of R, G, B. Explore and print the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[:,0:1,:3]  # selects all rows, 2nd column, and all slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[130][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are down to a single pixel\n",
    "img[130][3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing tensors: slice along each axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_img = img[:,:,0]\n",
    "G_img = img[:,:,1]\n",
    "B_img = img[:,:,2]\n",
    "plt.imshow(R_img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(G_img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(B_img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice matrix by scalar\n",
    "img = img * 2\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks 101: Starting with a Single Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a single node (also called a neuron).\n",
    "\n",
    "![perceptron](figs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some inputs $x$, which get combined into an auxilliary variable $z$.  The auxilliary variable is passed through the activation function $\\sigma\\left(z\\right)$ and the result is the output.\n",
    "\n",
    "Here is another image showing each step.\n",
    "\n",
    "![](figs/expanded-perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the inputs are linearly combined according to some weights $w$ and a bias $b$.  This transformation is also sometimes called an **affine** transformation.  The perceptron transforms the weighted inputs according to the rule of the activation function.  For a single perceptron, the output $y$ is just the output from the perceptron.  The linear transformation and activation of the neuron occurs within a single **layer** of the network (shown in the dotted box)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "The task is to approximate (or learn) a function $f\\left(x\\right)$ given some input $x$.  For demonstration purposes, the function we will try to learn is a Gaussian function \n",
    "\\begin{align}\n",
    "f\\left(x\\right) = e^{-x^{2}}\n",
    "\\textrm{}\n",
    "\\end{align}\n",
    "\n",
    "Even though we represent the input $x$ as a vector on the computer, you should think of it as a single input.  For example, we're not passing $x$ and $y$ into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "x[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the sigmoid and the tanh to see their difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):                                        \n",
    "    return 1 / (1 + np.exp(-x)) # The real function\n",
    "\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, np.tanh(x), label='tanh')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform a $tanh$ into a $sigmoid$:\n",
    "\n",
    "\\begin{align}\n",
    "\\dfrac{1}{2}+\\dfrac{1}{2}\\tanh(\\dfrac{x-c}{a}) = \\dfrac{1}{2}+\\dfrac{1}{2}\\dfrac{e^{(x-c)/a} - e^{- (x-c)/a}}{e^{(x-c)/a)} + e^{- (x-c)/a}}=\\dfrac{1}{1+e^{-2(x-c)/a}} \\qquad\\text{(1.2)}\n",
    "\\textrm{}\n",
    "\\end{align}\n",
    "<BR>\n",
    "($c$ is the threshold, $c$=0 in our case, and $a$ is the sharpness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create and plot the dataset for our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # The real function\n",
    "\n",
    "plt.plot(x, f, label='gaussian')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the single-layer, single neuron network give us.  We have a couple of choices to make:\n",
    "1. We must choose some weights and some biases\n",
    "2. We must choose an activation function\n",
    "\n",
    "For now, we will manually specify the weights and biases.\n",
    "\n",
    "We choose a *sigmoid* activation function $$\\sigma\\left(x\\right) = \\dfrac{1}{1 + e^{-z}}.$$  What are the limits $\\displaystyle\\lim_{z\\to\\infty}\\sigma\\left(z\\right)$ and $\\displaystyle\\lim_{z\\to-\\infty}\\sigma\\left(z\\right)$?  Actually, the sigmoid we have here is called the *logistic* function.  Sigmoids are really a family of functions and the logistic function is just one member in that family.\n",
    "\n",
    "There are other activation functions as well.  For example:\n",
    "* Rectified linear unit:  $$\\sigma\\left(z\\right) = \\text{max}\\left(0, z\\right)$$\n",
    "* Hyperbolic tangent:  $$\\sigma\\left(z\\right) = \\tanh\\left(z\\right)$$\n",
    "\n",
    "Choosing the correct activation function is a really big deal.  Notice that the activation functions we've mentioned so far have the feature that they \"turn on\" at some point and \"saturate\" at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the weight and bias\n",
    "w = -4.5 # weight\n",
    "b = 3.0 # bias\n",
    "\n",
    "# Perceptron output\n",
    "z = w * x + b # Affine transformation\n",
    "h = 1.0 / (1.0 + np.exp(-z)) # Sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the activation function and the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(11,7)) # create axes object\n",
    "\n",
    "SIZE = 16\n",
    "# Plot\n",
    "ax.plot(x, f, ls='-.', lw=4, label=r'True function')\n",
    "ax.plot(x, h, lw=4, label=r'Single Neuron')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single perceptron simply turns the output on and off at some point, but that's about it.  We see that the neuron is on until about $x=0$ at which point it abruptly turns off.  It's able to get \"close\" to the true function.  Otherwise, it has nothing in common with the true function.\n",
    "\n",
    "What do you think will happen if you change $w$ and $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Write two `Python` functions:\n",
    "\n",
    "The first function should return an affine transformation of the data for a single neuron.  Here's the required interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def affine(x, w, b):\n",
    "    \"\"\"Return affine transformation of x\n",
    "    \n",
    "    INPUTS\n",
    "    ======\n",
    "    x: A numpy array of points in x\n",
    "    w: A float representing the weight of the perceptron\n",
    "    b: A float representing the bias of the perceptron\n",
    "    \n",
    "    RETURN\n",
    "    ======\n",
    "    z: A numpy array of points after the affine transformation\n",
    "       z = wx + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Code goes here\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function should return the sigmoid activation function.  Here's the required interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def sigmoid(z):\n",
    "    # Code goes here\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:  Using your two functions, recreate the example from class and try to change the weight and bias to get a better fit\n",
    "\n",
    "##### Comments\n",
    "* We say that the activation occurs when $\\sigma = \\dfrac{1}{2}$.  We can show that this corresponds to $x = -\\dfrac{b}{w}$.\n",
    "* The \"steepness\" of the sigmoid is controlled by $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def affine(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # data\n",
    "\n",
    "w = 1.5\n",
    "b = -2.0\n",
    "\n",
    "h = sigmoid(affine(x, w, b))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(11,7))\n",
    "ax.plot(x, f, lw=4, label=r'True function')\n",
    "ax.plot(x, h, lw=4, label=r'Single Neuron')\n",
    "ax.axhline(y=0.5, linestyle='-.')\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc=2) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Observation\n",
    "Notice that we wrote the output as `sigmoid(affine(x))`.  This was not a coincidence.  It looks like a composition of functions.  In fact, that is what a neural network is doing.  It's building up an approximation to a function by creating a composition of functions.  For example, a composition of three functions would be written as $$\\varphi_{3}\\left(\\varphi_{2}\\left(\\varphi_{1}\\left(x\\right)\\right)\\right).$$\n",
    "\n",
    "What happens if we play with the weights and biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [-5.0, 0.1, 5.0] # Create a list of weights\n",
    "b = [0.0, -1.0, 1.0] # Create a list of biases\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "ax.plot(x, f, lw=4, ls='-.', label='True function')\n",
    "for wi, bi in zip(w, b):\n",
    "    h = sigmoid(affine(x, wi, bi))\n",
    "    ax.plot(x, h, lw=4, label=r'$w = {0}$, $b = {1}$'.format(wi,bi))\n",
    "ax.set_title('Single neuron network', fontsize=SIZE)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do an exhaustive search of the weights and biases, but it sure looks like this single perceptron is never going to match the actual function.  Again, we shouldn't be suprised about this.  The output layer of the network is simple the logistic function, which can only have so much flexibility.\n",
    "\n",
    "Let's try to make our network more flexible by using **more nodes**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Perceptrons in a Single Layer\n",
    "It appears that a single neuron is somewhat limited in what it can accomplish.  What if we expand the number of nodes/neurons in our network?  We have two obvious choices here.  One option is to add depth to the network by putting layers next to each other.  The other option is to stack neurons on top of each other in the same layer.  Now the network has some width, but is still only one layer deep.\n",
    "\n",
    "The following figure shows a single-layer network with two nodes in one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/multiple-perceptrons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some observations\n",
    "1. We still have a single input in this case.  Note that this is not necessary in general.  We're just keeping things simple with a single input for now. If we have more inputs we will have a matrix for $X$.\n",
    "2. Each node (or neuron) has a weight and bias associated with it.  An affine transformation is performed for each node.\n",
    "3. Both nodes use the same activation function form $\\sigma\\left(\\cdot\\right)$ on their respective inputs.\n",
    "4. The outputs of the nodes must be combined to give the overall output of the network.  There are a variety of ways of accomplishing this.  In the current example, we just take a linear combination of the node outputs to produce the actual prediction.  Notice that now we have weights and biases at the output too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens in this case.  First, we just compute the outputs of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # data\n",
    "\n",
    "w = np.array([1.5, -2.5])\n",
    "b = np.array([0.4, -2.0])\n",
    "\n",
    "# Affine transformations\n",
    "z1 = w[0] * x + b[0]\n",
    "z2 = w[1] * x + b[1]\n",
    "\n",
    "# Node outputs\n",
    "h1 = 1.0 / (1.0 + np.exp(-z1))\n",
    "h2 = 1.0 / (1.0 + np.exp(-z2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot things and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls = '-.', label='True function')\n",
    "ax.plot(x, h1, lw=4, label='First neuron')\n",
    "ax.plot(x, h2, lw=4, label='Second neuron')\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Comparison of Neuron Outputs', fontsize=SIZE)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we expected.  Some sigmoids.  Of course, to get the network prediction we must combine these two sigmoid curves somehow.  First we'll just add $h_{1}$ and $h_{2}$ without any weights to see what happens.\n",
    "\n",
    "#### Note\n",
    "We are **not** doing classification here.  We are trying to predict an actual function.  The sigmoid activation is convenient when doing classification because you need to go from $0$ to $1$.  However, when learning a function, we don't have as good of a reason to choose a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output\n",
    "wout = np.ones(2) # Set the output weights to unity to begin\n",
    "bout = 0.0 # No bias yet\n",
    "yout = wout[0] * h1 + wout[1] * h2 + bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, ls='-.', lw=4, label=r'True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = h_{1} + h_{2}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "* The network prediction is still not good. *But*, it is pretty sophisticated.  We just have two neurons, but we get some pretty interesting behavior. We didn't do anything with the output weights.  Those are probably important.  Now let's see what happens when we change the weights on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output\n",
    "wout = np.array([-1.5, -1.5])\n",
    "bout = np.array(1.5)\n",
    "\n",
    "yout = wout[0] * h1 + wout[1] * h2 + bout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls = '-.', label='True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = w_{1}^{o}h_{1} + w_{2}^{o}h_{2} + b^{o}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=SIZE) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=SIZE)\n",
    "\n",
    "ax.tick_params(labelsize=SIZE) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=SIZE, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool!  The two nodes interact with each other to produce a pretty complicated-looking function.  It still doesn't match the true function, but now we have some hope.  In fact, it's starting to look a little bit like a Gaussian!\n",
    "\n",
    "We can do better.  There are three obvious options at this point:\n",
    "1. Change the number of nodes\n",
    "2. Change the activation functions\n",
    "3. Change the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Mathematical Notation\n",
    "Before proceeding, let's learn a more succint way of doing the calculations.  If you have a network with a lot of nodes, then you probably don't want to manually determine the output of each node.  It will take forever.  Instead, you can package the computations up using a more compact notation.  We'll illustrate the ideas with the two-node network.\n",
    "\n",
    "Suppose we have a single input $x$ to a single-layer two-node network.  We can store the weights from each node in a vector $\\mathbf{w} \\in \\mathbb{R}^{2}$.  Similarly, we can store the biases from each node in a vector $\\mathbf{b} \\in \\mathbb{R}^{2}$.  The affine transformation is then written as $$\\mathbf{z} = \\mathbf{w}x + \\mathbf{b}$$ where the usual laws of vector addition and multiplication by a scalar apply.  Of course, we have that $\\mathbf{z} \\in \\mathbb{R}^{2}$ as well.  Next we evaluate the output from each node.  Formally, we write $$\\mathbf{h} = \\sigma\\left(\\mathbf{z}\\right)$$ where, once again, $\\mathbf{h}\\in\\mathbb{R}^{2}$.  Moreover, it is *understood* that $\\sigma$ operates on each individual element of $\\mathbf{z}$ separately.  If we denote each component of $\\mathbf{z}$ by $z_{j}, \\quad j = 1, 2$ then we can write $$h_{j} = \\sigma\\left(z_{j}\\right), \\quad j = 1, 2.$$\n",
    "\n",
    "Lastly, we must do something about the output layer.  Mathematically we write $$y_{out} = \\mathbf{w}_{out} \\cdot \\mathbf{h} + b_{out}$$ where $\\mathbf{w}_{out} \\in \\mathbb{R}^{2}$ and $b_{out} \\in \\mathbb{R}$.\n",
    "\n",
    "### Comments on the Implementation\n",
    "Mathematically, this all makes perfect sense.  There is a slight wrinkle when we get to the implementation.  The reason is that $x$ must be stored as a vector (or array) on the computer!  When we do that, we must be very careful in telling the computer how to perform the multiplications.\n",
    "\n",
    "#### Example A\n",
    "Suppose we have just stored three points in $x$ on the computer and store $x$ in an array.  Then we have \n",
    "\\begin{align}\n",
    "  x = \\begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\end{bmatrix}.\n",
    "\\end{align}\n",
    "When we write $\\mathbf{w}x + \\mathbf{b}$ the computer actually doesn't know what multiplication we want to do.  If we treat $x$ as though it has dimension $3 \\times 1$ then simply multiplying $\\mathbf{w}$ by $x$ will result in a dimension mismatch error.\n",
    "\n",
    "#### Example B\n",
    "What if we're at the output of the network?  Then $\\mathbf{h}$ is stored as a $2 \\times \\text{num_points}$ array.  But $\\mathbf{w}_{out}$ is a $2\\times 1$ array!  To get the dot product right, we need to tell the computer to take the dot product on each column.  Here's the situation:\n",
    "\\begin{align}\n",
    "  \\mathbf{w}_{out} \\cdot \\mathbf{h} &= \n",
    "  \\begin{bmatrix}\n",
    "    w_{1}^{out} \\ \\ w_{2}^{out}\n",
    "  \\end{bmatrix}^{T}\n",
    "  \\begin{bmatrix}\n",
    "    h_{11} & h_{12} & h_{13} \\\\\n",
    "    h_{21} & h_{22} & h_{23}\n",
    "  \\end{bmatrix}\n",
    "  \\\\\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "    w_{1}^{out}h_{11} + w_{2}^{out}h_{21} \\qquad w_{1}^{out}h_{12} + w_{2}^{out}h_{22} \\qquad w_{1}^{out}h_{13} + w_{2}^{out}h_{23}\n",
    "  \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Handling Missing Variables\n",
    "\n",
    "#### Imputation in pandas\n",
    "\n",
    "Pandas has chosen to use the floating point NaN internally to denote missing data and that was largely for simplicity and performance reasons. The library will also recognize the Python `None` as “missing” or “not available” or “NA”. When doing descriptive statistics, pandas excludes missing values by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([0, 1, np.nan, 4, 6, 8, 15, 23, 25, np.nan])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for missing values\n",
    "s.isna() # or s.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas can [interpolate](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate) according to different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many?\n",
    "s.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with zeros\n",
    "s.fillna(0)\n",
    "# s['one'].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may wish to simply exclude labels from a data set which refer to missing data. To do this, use \n",
    "s = pd.Series([0, 1, np.nan, 4, 6, 8, 15, 23, 25, np.nan])\n",
    "s = s.dropna()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([0, 1, np.nan, 4, 6, 8, 15, 23, 25, np.nan])\n",
    "s.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(copy=True, missing_values=np.nan, strategy='mean')\n",
    "s1 = pd.Series([0, 1, np.nan, 4, 6, 8, 15, 23, 25, np.nan])\n",
    "s2 = pd.Series([0, 1, np.nan, 4, 6, 8, 15, 23, 25, np.nan])\n",
    "d = {'col1': s1, 'col2': s2}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.fit(df) \n",
    "df = imp.transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleImputer(copy=True, fill_value=None, missing_values=np.nan, strategy='mean', verbose=0)\n",
    "print(imp.transform(df))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp2 = SimpleImputer(strategy=\"most_frequent\")\n",
    "s1 = pd.Series(['eleni', 'john', np.nan, 'john', 'mary', 'john', 'george', 'eleni', 'john', np.nan])\n",
    "d = {'name': s1}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp2.fit(df) \n",
    "df = imp2.transform(df)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
